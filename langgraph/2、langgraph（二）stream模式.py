import os
import asyncio
import time
from typing import Annotated, List, Generator
from typing_extensions import TypedDict
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.config import get_stream_writer  
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
from langchain_tavily._utilities import TavilySearchAPIWrapper
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command

load_dotenv()
llm = ChatOpenAI(
        model_name="Qwen3-235B-A22B",
        temperature=0.9,
        openai_api_base="http://10.1.18.99:8089/v1",
        openai_api_key="sk-dIl9oEE1SCJHXkzkdTmivPJgtxMGHNgvvNx5e17T4XYHBBOG",
    )

search_tool = TavilySearch(
    max_results=2,
    api_wrapper=TavilySearchAPIWrapper(
        tavily_api_key="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"
    )
)

tools = [search_tool]

# def add_messages(left: list[BaseMessage], right: list[BaseMessage]) -> list[BaseMessage]:
#     """å°†æ–°æ¶ˆæ¯åˆ—è¡¨è¿½åŠ åˆ°æ—§æ¶ˆæ¯åˆ—è¡¨ä¸­"""
#     return left + right

# class AgentState(TypedDict):
#     messages: Annotated[List[BaseMessage], add_messages]

# def agent_node(state: AgentState):
#     """æ€è€ƒèŠ‚ç‚¹ï¼šè°ƒç”¨ LLM å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
#     print("--- Executing node: agent_node ---")
#     response = llm.bind_tools(tools).invoke(state['messages'])
#     return {"messages": [response]}

# tool_node = ToolNode(tools=tools)

# def router(state: AgentState) -> str:
#     """è·¯ç”±ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·"""
#     print("--- Executing router ---")
#     last_message = state['messages'][-1]
#     if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
#         return "tool_node"
#     else:
#         return "END"

# graph_builder = StateGraph(AgentState)
# graph_builder.add_node("agent_node", agent_node)
# graph_builder.add_node("tool_node", tool_node)
# graph_builder.set_entry_point("agent_node")
# graph_builder.add_conditional_edges(
#     "agent_node", 
#     router,
#     {
#         "tool_node": "tool_node",
#         "END": END
#     }
# )
# graph_builder.add_edge("tool_node", "agent_node")

# app = graph_builder.compile()


# async def run_values_mode():
#     print("\n--- æ¨¡å¼: values ---")
#     inputs = {"messages": [HumanMessage(content="ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·?")]}
#     async for chunk in app.astream(inputs, stream_mode="values"):
#         print("--- çŠ¶æ€å¿«ç…§ ---")
#         print(chunk)
#         print("-" * 25)

# asyncio.run(run_values_mode())

# async def run_updates_mode():
#     print("\n--- æ¨¡å¼: updates ---")
#     inputs = {"messages": [HumanMessage(content="ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·?")]}
#     async for chunk in app.astream(inputs, stream_mode="updates"):
#         print(chunk)
#         print("-" * 25)

# asyncio.run(run_updates_mode())

# async def run_messages_mode():
#     print("\n--- æ¨¡å¼: messages ---")
#     inputs = {"messages": [HumanMessage(content="ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·?")]}
#     async for chunk in app.astream(inputs, stream_mode="messages"):
#         if chunk:
#             print(chunk[0].content, end="", flush=True)
#             print("-" * 25)

# asyncio.run(run_messages_mode())

# def agent_node_with_custom_event(state: AgentState):
#     print("--- Executing node: agent_node_with_custom_event ---")
#     writer = get_stream_writer()  
#     writer({"data": "Retrieved 0/100 records", "type": "progress"})  
#     # æ‰§è¡ŒæŸ¥è¯¢  
#     writer({"data": "Retrieved 100/100 records", "type": "progress"})  

#     time.sleep(1) 
#     response = llm.bind_tools(tools).invoke(state['messages'])
#     return {"messages": [response]}

# # 2. æ„å»ºå¹¶ç¼–è¯‘æ–°å›¾
# graph_custom_builder = StateGraph(AgentState)
# graph_custom_builder.add_node("agent_node", agent_node_with_custom_event)
# graph_custom_builder.add_node("tool_node", tool_node)
# graph_custom_builder.set_entry_point("agent_node")
# graph_custom_builder.add_conditional_edges(
#     "agent_node", 
#     router,
#     {
#         "tool_node": "tool_node",
#         "END": END
#     }
# )
# graph_custom_builder.add_edge("tool_node", "agent_node")
# app_custom = graph_custom_builder.compile()

# # 3. è¿è¡Œå¹¶ç›‘å¬
# async def run_custom_mode():
#     print("\n--- æ¨¡å¼: custom ---")
#     inputs = {"messages": [HumanMessage(content="ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·?")]}
#     async for chunk in app_custom.astream(inputs, stream_mode="custom"):
#         print(chunk)

# asyncio.run(run_custom_mode())

# # 1. ç¼–è¯‘ä¸€ä¸ªåœ¨ agent_node ä¹‹åä¼šä¸­æ–­çš„ app

# checkpointer = MemorySaver()
# app_interrupt = graph_builder.compile(checkpointer=checkpointer,interrupt_after=["agent_node"])

# async def run_interrupt_mode_correctly():
#     print("\n--- Correctly Handling Interrupts ---")
#     # thread_id å°±åƒä¸€ä¸ªâ€œå­˜æ¡£IDâ€ï¼Œè®©æˆ‘ä»¬å¯ä»¥æ¢å¤å›¾çš„çŠ¶æ€
#     config = {"configurable": {"thread_id": "interrupt-thread-1"}}
#     inputs = {"messages": [HumanMessage(content="ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·?")]}
#     print("--- First run, streaming with 'values', expecting interrupt ---")
#     # ä½¿ç”¨ "values" æ¨¡å¼è¿è¡Œï¼Œæµä¼šåœ¨ä¸­æ–­ç‚¹è‡ªåŠ¨ç»“æŸ
#     async for chunk in app_interrupt.astream(inputs, config=config, stream_mode="updates"):
#         print("--- Stream Chunk ---")
#         print(chunk)
    
#     print("\n--- [Graph Interrupted] ---")

#     # æ£€æŸ¥å½“å‰çŠ¶æ€ï¼Œç¡®è®¤æˆ‘ä»¬æ­£å¤„äºä¸­æ–­çŠ¶æ€
#     current_state = await app_interrupt.aget_state(config)
#     print("\nLast message before interrupt:", current_state.values['messages'][-1])
#     # `next` å±æ€§å‘Šè¯‰æˆ‘ä»¬ä¸‹ä¸€æ­¥å°†æ‰§è¡Œå“ªä¸ªèŠ‚ç‚¹ï¼Œè¿™è¯æ˜å›¾å·²æš‚åœ
#     print("Next step would be:", current_state.next)

#     if current_state.next: # å¦‚æœ next æœ‰å€¼ï¼Œè¯´æ˜å›¾è¢«ä¸­æ–­äº†
#         print("\n--- Resuming execution ---")
#         # ä¼ å…¥ None å¹¶ä½¿ç”¨ç›¸åŒçš„ config æ¥ç»§ç»­æ‰§è¡Œ
#         async for chunk in app_interrupt.astream(None, config=config, stream_mode="updates"):
#             print("--- Resumed Chunk ---")
#             print(chunk)
#             print("-" * 25)

# asyncio.run(run_interrupt_mode_correctly())


# å®šä¹‰ä¸€ä¸ªåŒ…å«ç”Ÿæˆå†…å®¹çš„çŠ¶æ€
# class GenerationState(TypedDict):
#     messages: list
#     generation: str

# def generator_node(state: GenerationState):
#     print(">>> æ­£åœ¨æ‰§è¡Œ: generator_node")
#     return {"generation": "è¿™æ˜¯ä¸€ä¸ªåˆæ­¥ç”Ÿæˆçš„ç­”æ¡ˆã€‚"}

# def validator_node(state: GenerationState):
#     print(">>> æ­£åœ¨æ‰§è¡Œ: validator_node")
#     quality_score = random.random()
#     print(f"ç­”æ¡ˆè´¨é‡åˆ†: {quality_score:.2f}")
#     if quality_score > 0.7:
#         print("--- å†³ç­–: è´¨é‡åˆæ ¼ï¼Œç›´æ¥ç»“æŸ ---")
#         return Command(goto=END)
#     else:
#         print("--- å†³ç­–: è´¨é‡ä¸åˆæ ¼ï¼Œå¼ºåˆ¶è·³è½¬åˆ°äººå·¥å®¡æ ¸ ---")
#         return Command(goto="human_review")

# def human_review_node(state: GenerationState):
#     print(">>> æ­£åœ¨æ‰§è¡Œ: human_review_node (éœ€è¦äººå·¥ä»‹å…¥ï¼)")
#     return Command(update={"generation": state['generation'] + " [ç»è¿‡äººå·¥ä¼˜åŒ–]"})

# # æ„å»ºå›¾ï¼šæ³¨æ„ï¼Œæˆ‘ä»¬æ•…æ„ä¸ç”» validator åˆ° human_review çš„è¾¹
# graph_dynamic = StateGraph(GenerationState)
# graph_dynamic.add_node("generator", generator_node)
# graph_dynamic.add_node("validator", validator_node)
# graph_dynamic.add_node("human_review", human_review_node)
# graph_dynamic.set_entry_point("generator")
# graph_dynamic.add_edge("generator", "validator")
# graph_dynamic.add_edge("human_review", "validator") # å®¡æ ¸åå†æ¬¡éªŒè¯

# app_dynamic = graph_dynamic.compile()

# # å¤šæ¬¡è¿è¡Œï¼Œä½ ä¼šçœ‹åˆ°å®ƒæœ‰æ—¶ç›´æ¥ç»“æŸï¼Œæœ‰æ—¶ä¼šè¿›å…¥ human_review
# print("\n--- è¿è¡ŒåŠ¨æ€è·³è½¬å›¾ ---")
# result = app_dynamic.invoke({"messages":[]})
# print(result['generation'])

tool_node = ToolNode(tools=tools)
from functools import partial

def add_messages(left: list[BaseMessage], right: list[BaseMessage],k: int = 10) -> list[BaseMessage]:
    """å°†æ–°æ¶ˆæ¯åˆ—è¡¨è¿½åŠ åˆ°æ—§æ¶ˆæ¯åˆ—è¡¨ä¸­"""
    full_list = left + right
    return full_list[-k:]



class ResearchState(TypedDict):
    messages: Annotated[list[BaseMessage], partial(add_messages, k=10)]
    search_results: list | None
    draft_report: str | None


async def generate_draft_node(state: ResearchState):
    """æ ¹æ®æœç´¢ç»“æœç”ŸæˆæŠ¥å‘Šåˆç¨¿"""
    print("--- Executing node: generate_draft_node (AIæ­£åœ¨æ’°å†™åˆç¨¿...) ---")
    prompt = f"æ ¹æ®ä»¥ä¸‹æœç´¢ç»“æœï¼Œä¸ºç”¨æˆ·çš„æœ€åä¸€ä¸ªé—®é¢˜ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æŠ¥å‘Šåˆç¨¿: {state['search_results']}"
    messages = state["messages"] + [("user", prompt)]
    
    response = await llm.ainvoke(messages)
    
    return {"draft_report": response.content}

def human_review_node(state: ResearchState):
    """äººç±»å®¡æ ¸èŠ‚ç‚¹ - è¿™ä¸ªèŠ‚ç‚¹æœ¬èº«ä¸æ‰§è¡Œé€»è¾‘ï¼Œä»…ä½œä¸ºä¸­æ–­ç‚¹"""
    print("--- Reached node: human_review_node (ç­‰å¾…äººç±»å®¡æ ¸...) ---")
    return {}

def finalize_report_node(state: ResearchState):
    """æ ¹æ®ï¼ˆå¯èƒ½è¢«ä¿®æ”¹è¿‡çš„ï¼‰åˆç¨¿ç”Ÿæˆæœ€ç»ˆæ¶ˆæ¯"""
    print("--- Executing node: finalize_report_node (ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...) ---")
    reviewed_report = state["draft_report"]
    final_message = AIMessage(content=f"è¿™æ˜¯æ ¹æ®æ‚¨çš„å®¡æ ¸ç”Ÿæˆçš„æœ€ç»ˆæŠ¥å‘Šï¼š\n\n{reviewed_report}")
    return {"messages": [final_message]}

def quality_check_node(state: ResearchState):
    """
    æ£€æŸ¥æœç´¢ç»“æœçš„è´¨é‡ï¼Œå¹¶ä½¿ç”¨ goto åŠ¨æ€å†³å®šä¸‹ä¸€æ­¥ã€‚
    """
    print("--- Executing node: quality_check_node ---")
    writer = get_stream_writer()
    writer({"status": "æ­£åœ¨è¯„ä¼°æœç´¢ç»“æœè´¨é‡...","type":"quality_check_node"})

    search_results = state.get("search_results")
    if not search_results or len(search_results) < 2:
        print("--- å†³ç­–: æœç´¢ç»“æœä¸è¶³ï¼Œä¸­æ–­å¹¶è¯·æ±‚ç”¨æˆ·æ¾„æ¸… ---")
        return Command(goto="clarify_with_user_node")
    else:
        print("--- å†³ç­–: æœç´¢ç»“æœå……è¶³ï¼Œè·³è½¬è‡³æŠ¥å‘Šç”Ÿæˆ ---")
        return Command(goto="generate_draft_node")


def clarify_with_user_node(state: ResearchState):
    """ç”Ÿæˆéœ€è¦ç”¨æˆ·æ¾„æ¸…çš„æç¤ºæ¶ˆæ¯"""
    clarify_msg = AIMessage(content="æœç´¢ç»“æœä¸è¶³ï¼Œè¯·æä¾›æ›´å…·ä½“çš„é—®é¢˜æˆ–è¡¥å……ä¿¡æ¯ã€‚")
    return {"messages": [clarify_msg]}

def agent_node(state: ResearchState):
    """æ€è€ƒèŠ‚ç‚¹ï¼šè°ƒç”¨ LLM å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
    response = llm.bind_tools(tools).invoke(state['messages'])
    return {"messages": [response]}

def tool_executor(state: ResearchState):
    """æ‰§è¡Œå·¥å…·å¹¶æå–ç»“æœåˆ° search_results"""
    tool_node = ToolNode(tools=tools)
    tool_output = tool_node.invoke(state)
    tool_results = []
    for msg in tool_output["messages"]:
        if isinstance(msg, ToolMessage):
            import json
            tool_results.append(json.loads(msg.content))
    return {
        # "messages": tool_output["messages"],
        "search_results": tool_results
    }
    
def router(state: ResearchState) -> str:
    """è·¯ç”±ï¼šåˆ¤æ–­ä¸‹ä¸€æ­¥æ˜¯è°ƒç”¨å·¥å…·ã€ç›´æ¥ç»“æŸè¿˜æ˜¯è¿›å…¥è´¨é‡æ£€æŸ¥"""
    last_message = state['messages'][-1]
    
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tool_node"
    else:
        return "END"

final_graph_builder = StateGraph(ResearchState)

final_graph_builder.add_node("agent_node", agent_node)
final_graph_builder.add_node("tool_node", tool_executor)
final_graph_builder.add_node("quality_check_node", quality_check_node)
final_graph_builder.add_node("clarify_with_user_node", clarify_with_user_node)

final_graph_builder.add_node("generate_draft_node", generate_draft_node)
final_graph_builder.add_node("human_review_node", human_review_node)
final_graph_builder.add_node("finalize_report_node", finalize_report_node)

final_graph_builder.add_edge("generate_draft_node", "human_review_node")
final_graph_builder.add_edge("human_review_node", "finalize_report_node")
final_graph_builder.add_edge("finalize_report_node", END)

final_graph_builder.set_entry_point("agent_node")

final_graph_builder.add_conditional_edges(
    "agent_node", 
    router,
    {
        "tool_node": "tool_node",
        "END": END
    }
)


final_graph_builder.add_edge("tool_node", "quality_check_node")
final_graph_builder.add_edge("clarify_with_user_node", END)
final_checkpointer = MemorySaver()
app = final_graph_builder.compile(
    checkpointer=final_checkpointer,
    interrupt_before=["human_review_node", "clarify_with_user_node"],
)

async def run_collaborative_session():
    config = {"configurable": {"thread_id": "collab-thread-2"}}
    inputs = {"messages": [HumanMessage(content="å¯¹æ¯”ä¸€ä¸‹LangGraphå’Œä¼ ç»Ÿçš„LangChain Agentåœ¨å®ç°å¤æ‚å·¥ä½œæµæ—¶çš„ä¼˜åŠ£åŠ¿")]}

    print("--- [Session Start] ---")
    # 1. å¯åŠ¨å›¾ï¼Œå®ƒå°†è¿è¡Œç›´åˆ°ç¬¬ä¸€ä¸ªä¸­æ–­ç‚¹
    async for output in app.astream(inputs, config=config):
        for key, value in output.items():
            print(f"Node '{key}' output: {value}")
    
    # 2. æ£€æŸ¥ä¸­æ–­çŠ¶æ€
    current_state = await app.aget_state(config)
    
    # æ£€æŸ¥æ˜¯å¦åœ¨ human_review_node ä¸­æ–­
    if "human_review_node" in current_state.next:
        print("\nğŸš¦ --- [Graph Interrupted for Human Review] --- ğŸš¦")
        
        # 3. ä»çŠ¶æ€ä¸­æå–AIç”Ÿæˆçš„åˆç¨¿
        draft_report = current_state.values.get("draft_report")
        
        # --- [API Boundary] Backend å°†åˆç¨¿å‘é€ç»™ Frontend ---
        print("\nAI ç”Ÿæˆçš„æŠ¥å‘Šåˆç¨¿ï¼š")
        print("--------------------")
        print(draft_report)
        print("--------------------")
        
        # 4. æ¨¡æ‹Ÿç”¨æˆ·åœ¨å‰ç«¯é¡µé¢è¿›è¡Œä¿®æ”¹
        print("\nè¯·åœ¨ä¸‹æ–¹ç¡®è®¤æˆ–ä¿®æ”¹æŠ¥å‘Šå†…å®¹ã€‚å¦‚æœæ— éœ€ä¿®æ”¹ï¼Œç›´æ¥æŒ‰å›è½¦ã€‚")
        user_feedback = input("æ‚¨çš„ä¿®æ”¹ç‰ˆæœ¬: ")
        
        # å¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥ï¼Œåˆ™ä½¿ç”¨åŸå§‹åˆç¨¿
        if not user_feedback.strip():
            final_draft = draft_report
            print("--- ç”¨æˆ·å·²ç¡®è®¤ï¼Œä½¿ç”¨åŸå§‹åˆç¨¿ç»§ç»­ ---")
        else:
            final_draft = user_feedback
            print("--- ç”¨æˆ·å·²æäº¤ä¿®æ”¹ï¼Œä½¿ç”¨æ–°ç‰ˆæœ¬ç»§ç»­ ---")
            
        resume_inputs = {"draft_report": final_draft}
        
        print("\n--- [Session Resumed] ---")
        async for output in app.astream(resume_inputs, config=config):
            for key, value in output.items():
                print(f"Node '{key}' output: {value}")

    # 6. è·å–å¹¶æ‰“å°æœ€ç»ˆç»“æœ
    final_state = await app.aget_state(config)
    if not final_state.next:
        final_message = final_state.values["messages"][-1]
        print(final_message.content)

if __name__ == "__main__":
    asyncio.run(run_collaborative_session())
