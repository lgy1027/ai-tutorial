import json
import os
import torch
import random
from sentence_transformers import SentenceTransformer, util

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# ä½ çš„å¾®è°ƒæ¨¡å‹è·¯å¾„
FINETUNED_MODEL_PATH = os.path.join(CURRENT_DIR, "output_model_final")
# åŸºåº§æ¨¡å‹åç§°
BASE_MODEL_NAME = "BAAI/bge-large-zh-v1.5"
# æ•°æ®é›†è·¯å¾„
DATA_FILE = os.path.join(CURRENT_DIR, "finetune_data_mined.jsonl")

# BGE æŒ‡ä»¤
QUERY_INSTRUCTION = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"

def load_test_cases(file_path, num_cases=3):
    """ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–å‡ ä¸ªæ¡ˆä¾‹"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data.append(json.loads(line))
            except:
                pass
    return random.sample(data, min(num_cases, len(data)))

def compare_models():
    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹ (è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
    
    # åŠ è½½åŸºåº§æ¨¡å‹ (CPUè·‘æ¨ç†å°±å¤Ÿäº†ï¼Œä¸ç”¨GPUä¹Ÿè¡Œ)
    base_model = SentenceTransformer(BASE_MODEL_NAME)
    
    # åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
    if not os.path.exists(FINETUNED_MODEL_PATH):
        print(f"âŒ æ‰¾ä¸åˆ°å¾®è°ƒæ¨¡å‹: {FINETUNED_MODEL_PATH}")
        return
    ft_model = SentenceTransformer(FINETUNED_MODEL_PATH)
    
    # è·å–æµ‹è¯•ç”¨ä¾‹
    test_cases = load_test_cases(DATA_FILE, num_cases=3)
    
    print("\n" + "="*50)
    print("ğŸš€ å¾®è°ƒæ•ˆæœå¤§æ¯”æ‹¼")
    print("="*50)

    for i, case in enumerate(test_cases):
        query = case['query']
        pos_doc = case['pos'][0]
        # æŒ‘é€‰ä¸€ä¸ªæœ€éš¾çš„è´Ÿä¾‹ (Hard Negative)
        neg_doc = case['neg'][0] if case.get('neg') else "æ— è´Ÿä¾‹æ•°æ®"
        
        print(f"\nğŸ“„ [æ¡ˆä¾‹ {i+1}] ç”¨æˆ·æé—®: {query}")
        print(f"âœ… æ­£ç¡®ç­”æ¡ˆç‰‡æ®µ (Positive): {pos_doc[:30]}...")
        print(f"âŒ ç›¸ä¼¼å¹²æ‰°ç‰‡æ®µ (Negative): {neg_doc[:30]}...")
        
        # --- 1. åŸºåº§æ¨¡å‹æ‰“åˆ† ---
        # æ³¨æ„ï¼šQuery è¦åŠ æŒ‡ä»¤ï¼ŒDoc ä¸éœ€è¦
        q_emb_base = base_model.encode(QUERY_INSTRUCTION + query, convert_to_tensor=True)
        p_emb_base = base_model.encode(pos_doc, convert_to_tensor=True)
        n_emb_base = base_model.encode(neg_doc, convert_to_tensor=True)
        
        score_pos_base = util.cos_sim(q_emb_base, p_emb_base).item()
        score_neg_base = util.cos_sim(q_emb_base, n_emb_base).item()
        
        # --- 2. å¾®è°ƒæ¨¡å‹æ‰“åˆ† ---
        q_emb_ft = ft_model.encode(QUERY_INSTRUCTION + query, convert_to_tensor=True)
        p_emb_ft = ft_model.encode(pos_doc, convert_to_tensor=True)
        n_emb_ft = ft_model.encode(neg_doc, convert_to_tensor=True)
        
        score_pos_ft = util.cos_sim(q_emb_ft, p_emb_ft).item()
        score_neg_ft = util.cos_sim(q_emb_ft, n_emb_ft).item()
        
        print("-" * 30)
        print(f"ğŸ¤– åŸºåº§æ¨¡å‹è¯„åˆ†:")
        print(f"   - æ­£ç¡®ç­”æ¡ˆç›¸ä¼¼åº¦: {score_pos_base:.4f}")
        print(f"   - å¹²æ‰°é¡¹ç›¸ä¼¼åº¦:   {score_neg_base:.4f}")
        diff_base = score_pos_base - score_neg_base
        print(f"   ğŸ‘‰ åŒºåˆ†åº¦ (æ­£-è´Ÿ): {diff_base:.4f} {'âš ï¸ å±é™©' if diff_base < 0.05 else ''}")
        
        print(f"ğŸ”¥ å¾®è°ƒæ¨¡å‹è¯„åˆ†:")
        print(f"   - æ­£ç¡®ç­”æ¡ˆç›¸ä¼¼åº¦: {score_pos_ft:.4f}")
        print(f"   - å¹²æ‰°é¡¹ç›¸ä¼¼åº¦:   {score_neg_ft:.4f}")
        diff_ft = score_pos_ft - score_neg_ft
        print(f"   ğŸ‘‰ åŒºåˆ†åº¦ (æ­£-è´Ÿ): {diff_ft:.4f} {'ğŸŒŸ ä¼˜ç§€' if diff_ft > diff_base else ''}")
        
        if diff_ft > diff_base:
            print("ğŸ“ˆ ç»“è®º: å¾®è°ƒåï¼Œå¹²æ‰°é¡¹è¢«æˆåŠŸæ¨è¿œï¼")
        else:
            print("ğŸ¤” ç»“è®º: æå‡ä¸æ˜æ˜¾ï¼Œå¯èƒ½æ˜¯è¯¥æ¡ˆä¾‹å¤ªç®€å•ã€‚")

if __name__ == "__main__":
    compare_models()
