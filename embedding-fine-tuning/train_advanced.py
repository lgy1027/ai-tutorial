import json
import os
import random
import torch
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.path.join(CURRENT_DIR, "finetune_data_mined.jsonl")
OUTPUT_DIR = os.path.join(CURRENT_DIR, "output_model_final")

# é€‰ç”¨ BGE-Large ä¸­æ–‡ç‰ˆä½œä¸ºåŸºåº§
MODEL_NAME = "BAAI/bge-large-zh-v1.5"
# BGE æ¨¡å‹ä¸“ç”¨çš„æŒ‡ä»¤å‰ç¼€ (å¿…é¡»åŠ ï¼Œå¦åˆ™æ•ˆæœå‡åŠ)
QUERY_INSTRUCTION = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"

# --- æ˜¾å­˜ä¼˜åŒ–é…ç½® (é˜² OOM æ ¸å¿ƒåŒº) ---
# æ‰¹æ¬¡å¤§å°ï¼šè®¾ä¸º 1ã€‚å¤šå¡æ¨¡å¼ä¸‹ï¼Œæ„å‘³ç€æ¯å¼ å¡ä¸€æ¬¡åªå¤„ç† 1 æ¡æ•°æ®ã€‚
# è™½ç„¶æ˜¯ 1ï¼Œä½†å› ä¸ºå¸¦äº†è´Ÿä¾‹ï¼Œå®é™…è®¡ç®—é‡ä¾ç„¶å¾ˆå¤§ï¼Œè¿™æ˜¯æœ€å®‰å…¨çš„è®¾ç½®ã€‚
BATCH_SIZE = 1 

# æœ€å¤§é•¿åº¦ï¼šä» 512 é™ä¸º 256ã€‚
# æ”¿åŠ¡/ä¸šåŠ¡æ–‡æ¡£é€šå¸¸ 256 ä¸ª token (çº¦ 400 å­—) è¶³å¤Ÿè¦†ç›–æ ¸å¿ƒè¯­ä¹‰ã€‚
# è¿™èƒ½ç›´æ¥èŠ‚çœ 50% ä»¥ä¸Šçš„æ˜¾å­˜ï¼
MAX_SEQ_LENGTH = 256 

# æœ€å¤§è´Ÿä¾‹æ•°ï¼šé™åˆ¶æ¯æ¡æ•°æ®åªç”¨å‰ 2 ä¸ªç¡¬è´Ÿä¾‹ã€‚
# ä¸è¦è´ªå¿ƒç”¨ 7 ä¸ªï¼Œå…ˆè·‘é€šæµç¨‹æœ€é‡è¦ã€‚
MAX_NEGS = 2 

# --- è®­ç»ƒå‚æ•° ---
NUM_EPOCHS = 3          # è®­ç»ƒ 3 è½®
LEARNING_RATE = 2e-5    # ç»å…¸å¾®è°ƒå­¦ä¹ ç‡
DEV_RATIO = 0.1         # åˆ’å‡º 10% çš„æ•°æ®ä½œä¸ºéªŒè¯é›†(è€ƒè¯•ç”¨)
# ==============================================================


def load_and_split_data(file_path, dev_ratio=0.1):
    """
    åŠ è½½æ•°æ®å¹¶éšæœºåˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    """
    all_data = []
    print(f"ğŸ“– [Step 1] æ­£åœ¨åŠ è½½åŸå§‹æ•°æ®: {file_path} ...")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {file_path}ï¼Œè¯·æ£€æŸ¥ generate_data.py æ˜¯å¦è¿è¡ŒæˆåŠŸï¼")

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                item = json.loads(line)
                # ç®€å•æ ¡éªŒæ•°æ®å®Œæ•´æ€§
                if item.get('query') and item.get('pos'):
                    all_data.append(item)
            except json.JSONDecodeError:
                pass
    
    print(f"ğŸ“Š å…±åŠ è½½ {len(all_data)} æ¡åŸå§‹æ•°æ®ã€‚")
    
    # éšæœºæ‰“ä¹±å¹¶åˆ‡åˆ†
    random.shuffle(all_data)
    split_idx = int(len(all_data) * (1 - dev_ratio))
    
    train_raw = all_data[:split_idx]
    dev_raw = all_data[split_idx:]
    
    print(f"âœ… æ•°æ®åˆ‡åˆ†å®Œæˆ: è®­ç»ƒé›† {len(train_raw)} æ¡ | éªŒè¯é›† {len(dev_raw)} æ¡")
    return train_raw, dev_raw

def convert_to_train_examples(raw_data):
    """
    å°† JSON æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯è¯»çš„ InputExample å¯¹è±¡
    åœ¨è¿™é‡Œè¿›è¡Œã€è´Ÿä¾‹æˆªæ–­ã€‘ä»¥èŠ‚çœæ˜¾å­˜
    """
    examples = []
    for data in raw_data:
        # ç»™ Query åŠ ä¸ŠæŒ‡ä»¤å‰ç¼€
        query = QUERY_INSTRUCTION + data['query']
        pos = data['pos'][0]
        neg_list = data.get('neg', [])
        
        # ã€æ ¸å¿ƒé˜²çˆ†æ˜¾å­˜é€»è¾‘ã€‘
        # åªå–å‰ MAX_NEGS ä¸ªè´Ÿä¾‹ã€‚
        # æœ€ç»ˆè¾“å…¥ = [Query, Pos, Neg1, Neg2] (å…± 4 ä¸ªå¥å­)
        texts = [query, pos] + neg_list[:MAX_NEGS]
        
        examples.append(InputExample(texts=texts))
    return examples

def create_evaluator(raw_data):
    """
    æ„å»ºè¯„ä¼°å™¨ï¼šæ¨¡æ‹ŸçœŸå®æ£€ç´¢è¿‡ç¨‹
    è®¡ç®— MRR@10 (å‰10åé‡Œæœ‰æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆ)
    """
    queries = {}
    corpus = {}
    relevant_docs = {}
    
    for idx, data in enumerate(raw_data):
        query_id = f"q_{idx}"
        pos_doc_id = f"doc_pos_{idx}"
        
        query_text = QUERY_INSTRUCTION + data['query']
        pos_text = data['pos'][0]
        
        queries[query_id] = query_text
        corpus[pos_doc_id] = pos_text
        relevant_docs[query_id] = {pos_doc_id}
        
        # æŠŠè´Ÿä¾‹ä¹ŸåŠ å…¥åˆ°â€œæ–‡åº“â€ä¸­ï¼Œå¢åŠ æ£€ç´¢éš¾åº¦ï¼Œæµ‹è¯•æ¨¡å‹åˆ†è¾¨èƒ½åŠ›
        for neg_idx, neg_text in enumerate(data.get('neg', [])):
            neg_doc_id = f"doc_neg_{idx}_{neg_idx}"
            corpus[neg_doc_id] = neg_text
            
    return evaluation.InformationRetrievalEvaluator(
        queries=queries,
        corpus=corpus,
        relevant_docs=relevant_docs,
        name='dev_eval',
        mrr_at_k=[10],   # å…³æ³¨å‰10å
        show_progress_bar=True
    )

def train_advanced():
    # æ£€æŸ¥ GPU
    if not torch.cuda.is_available():
        print("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è®­ç»ƒä¼šææ…¢ï¼")
    else:
        print(f"ğŸš€ æ£€æµ‹åˆ° {torch.cuda.device_count()} å¼ æ˜¾å¡ï¼Œå‡†å¤‡èµ·é£ï¼")

    # 1. åŠ è½½åŸºåº§æ¨¡å‹
    print(f"â¬‡ï¸ [Step 2] æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹: {MODEL_NAME} ...")
    model = SentenceTransformer(MODEL_NAME)
    
    # ã€æ˜¾å­˜ä¼˜åŒ– 1ã€‘å¼ºåˆ¶è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
    model.max_seq_length = MAX_SEQ_LENGTH
    print(f"ğŸ”§ å·²å°†æœ€å¤§åºåˆ—é•¿åº¦é™åˆ¶ä¸º: {MAX_SEQ_LENGTH}")

    # ã€æ˜¾å­˜ä¼˜åŒ– 2ã€‘å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)
    # è¿™ä¼šç‰ºç‰²ä¸€ç‚¹ç‚¹é€Ÿåº¦ï¼Œä½†èƒ½èŠ‚çœ 50%-70% çš„æ˜¾å­˜ï¼Œé˜²æ­¢ OOM çš„ç¥å™¨ï¼
    model.gradient_checkpointing_enable()
    print("ğŸ”§ å·²å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ¨¡å¼ (Gradient Checkpointing)")
    
    # 2. å‡†å¤‡æ•°æ®
    train_raw, dev_raw = load_and_split_data(DATA_FILE, DEV_RATIO)
    
    train_examples = convert_to_train_examples(train_raw)
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)
    
    # 3. å‡†å¤‡è¯„ä¼°å™¨
    print("ğŸ•µï¸ [Step 3] æ­£åœ¨æ„å»ºéªŒè¯é›†è¯„ä¼°å™¨...")
    evaluator = create_evaluator(dev_raw)
    
    # 4. å®šä¹‰æŸå¤±å‡½æ•°
    # ä½¿ç”¨å¤šè´Ÿä¾‹æ’åºæŸå¤± (Contrastive Loss çš„ä¸€ç§)
    train_loss = losses.MultipleNegativesRankingLoss(model=model, scale=20.0)
    
    # 5. å¼€å§‹è®­ç»ƒ
    print(f"ğŸš€ [Step 4] å¼€å§‹å¾®è°ƒè®­ç»ƒ (å…± {NUM_EPOCHS} è½®)...")
    print(f"   - Batch Size: {BATCH_SIZE} (Per GPU)")
    print(f"   - æ··åˆç²¾åº¦ (FP16): å¼€å¯")
    
    # è®¡ç®—è¯„ä¼°æ­¥æ•°ï¼šä¿è¯æ¯ä¸ª epoch è‡³å°‘è¯„ä¼°ä¸€æ¬¡
    total_steps = len(train_dataloader) * NUM_EPOCHS
    eval_steps = max(1, int(len(train_dataloader) * 0.5)) # æ¯åŠä¸ª epoch è¯„ä¼°ä¸€æ¬¡
    
    try:
        model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            evaluator=evaluator,
            evaluation_steps=eval_steps,
            epochs=NUM_EPOCHS,
            warmup_steps=int(total_steps * 0.1), # 10% æ­¥æ•°çƒ­èº«
            optimizer_params={'lr': LEARNING_RATE},
            output_path=OUTPUT_DIR,
            save_best_model=True,     # åªæœ‰æ•ˆæœå˜å¥½æ‰ä¿å­˜
            show_progress_bar=True,
            
            # ã€æ˜¾å­˜ä¼˜åŒ– 3ã€‘å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒ (FP16)
            # æ˜¾å­˜å†çœä¸€åŠï¼
            use_amp=True
        )
        print(f"\nğŸ‰ æ­å–œï¼è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("ğŸ’¡ å»ºè®®ï¼šå¦‚æœè¿˜æ˜¯ OOMï¼Œè¯·æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–è¿›ç¨‹å ç”¨æ˜¾å­˜ (ä½¿ç”¨ nvidia-smi æŸ¥çœ‹)")

if __name__ == "__main__":
    train_advanced()