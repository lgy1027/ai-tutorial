import json
import os
import random
import torch
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.path.join(CURRENT_DIR, "finetune_data_mined.jsonl")
OUTPUT_DIR = os.path.join(CURRENT_DIR, "output_model_3090")

MODEL_NAME = "BAAI/bge-large-zh-v1.5"
QUERY_INSTRUCTION = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"

# --- æ˜¾å­˜ä¸è´¨é‡å¹³è¡¡ (3090 ä¸“å±è°ƒä¼˜) ---

# 1. ç‰©ç† Batch Sizeï¼šæ˜¾å¡å®é™…æ¯æ¬¡å¤„ç†çš„æ•°é‡
# åœ¨å¼€å¯ Gradient Checkpointing å’Œ FP16 åï¼Œ3090 å¯ä»¥è½»æ¾å¤„ç† Batch=4 åˆ° 6
# è¿™é‡Œè®¾ä¸º 4 æ˜¯ç»å¯¹å®‰å…¨çš„ä¿å®ˆå€¼ (æ­é… 4-5 ä¸ªè´Ÿä¾‹)
PER_DEVICE_BATCH_SIZE = 4 

# 2. æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼šè¿™æ˜¯æå‡è´¨é‡çš„å…³é”®ï¼
# å®é™…ç­‰æ•ˆ Batch Size = 4 * 8 = 32
# è¾ƒå¤§çš„ç­‰æ•ˆ Batch Size èƒ½è®©æ¨¡å‹æ”¶æ•›æ›´ç¨³å®šï¼Œå­¦å¾—æ›´å¥½
GRADIENT_ACCUMULATION_STEPS = 8

# 3. åºåˆ—é•¿åº¦ï¼š256 å¯¹äºä¸šåŠ¡æ–‡æ¡£è¶³å¤Ÿï¼Œä¸”éå¸¸çœæ˜¾å­˜
MAX_SEQ_LENGTH = 256

# 4. è´Ÿä¾‹æ•°é‡ï¼šå»ºè®® 3-5 ä¸ªã€‚æ—¢èƒ½æä¾›è¶³å¤Ÿçš„éš¾æ ·æœ¬ï¼Œåˆä¸ä¼šæ’‘çˆ†æ˜¾å­˜ã€‚
MAX_NEGS = 4

# --- è®­ç»ƒå‚æ•° ---
NUM_EPOCHS = 3          # å‡ åä¸‡æ•°æ®çš„è¯ï¼Œ1-2 ä¸ª Epoch å¯èƒ½å°±å¤Ÿäº†ï¼Œçœ‹ loss
LEARNING_RATE = 2e-5    # ç»å…¸å­¦ä¹ ç‡
DEV_RATIO = 0.05        # æ•°æ®å¤šçš„è¯ï¼ŒéªŒè¯é›†æ¯”ä¾‹å¯ä»¥è°ƒå°ç‚¹ï¼Œæ¯”å¦‚ 5%
# =========================================================


def load_and_split_data(file_path, dev_ratio=0.05):
    """åŠ è½½å¹¶åˆ‡åˆ†æ•°æ®"""
    all_data = []
    print(f"ğŸ“– æ­£åœ¨åŠ è½½æµ·é‡æ•°æ®: {file_path} ...")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        # å¦‚æœæ•°æ®é‡çœŸçš„éå¸¸å¤§(æ¯”å¦‚ä¸ŠG)ï¼Œå»ºè®®ä½¿ç”¨ HuggingFace Datasets çš„ stream æ¨¡å¼
        # è¿™é‡Œå‡è®¾å‡ ç™¾å…†çš„ jsonl æ–‡ä»¶ï¼Œå†…å­˜èƒ½è£…ä¸‹
        for i, line in enumerate(f):
            try:
                item = json.loads(line)
                if item.get('query') and item.get('pos'):
                    all_data.append(item)
            except:
                pass
            
            # æ‰“å°è¿›åº¦ï¼Œé˜²æ­¢ç”¨æˆ·ä»¥ä¸ºå¡æ­»äº†
            if (i + 1) % 10000 == 0:
                print(f"   å·²åŠ è½½ {i + 1} è¡Œ...")

    print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(all_data)} æ¡")
    
    # éšæœºæ‰“ä¹±
    random.shuffle(all_data)
    split_idx = int(len(all_data) * (1 - dev_ratio))
    return all_data[:split_idx], all_data[split_idx:]

def convert_to_train_examples(raw_data):
    """è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬"""
    examples = []
    for data in raw_data:
        query = QUERY_INSTRUCTION + data['query']
        pos = data['pos'][0]
        neg_list = data.get('neg', [])
        
        # æˆªæ–­è´Ÿä¾‹ï¼Œé˜²æ­¢ OOM
        texts = [query, pos] + neg_list[:MAX_NEGS]
        examples.append(InputExample(texts=texts))
    return examples

def create_evaluator(raw_data):
    """æ„å»ºéªŒè¯é›†è¯„ä¼°å™¨ (ä¸ºäº†é€Ÿåº¦ï¼Œåªå–éªŒè¯é›†çš„å‰ 1000 æ¡è¿›è¡Œè¯„ä¼°)"""
    # å¦‚æœéªŒè¯é›†å¤ªå¤§ï¼Œè¯„ä¼°ä¼šéå¸¸æ…¢ï¼Œè¿™é‡Œåšä¸€ä¸ªæˆªæ–­
    EVAL_LIMIT = 1000 
    data_subset = raw_data[:EVAL_LIMIT]
    
    queries = {}
    corpus = {}
    relevant_docs = {}
    
    for idx, data in enumerate(data_subset):
        query_id = f"q_{idx}"
        pos_doc_id = f"doc_pos_{idx}"
        query_text = QUERY_INSTRUCTION + data['query']
        pos_text = data['pos'][0]
        
        queries[query_id] = query_text
        corpus[pos_doc_id] = pos_text
        relevant_docs[query_id] = {pos_doc_id}
        
        for neg_idx, neg_text in enumerate(data.get('neg', [])[:MAX_NEGS]):
            neg_doc_id = f"doc_neg_{idx}_{neg_idx}"
            corpus[neg_doc_id] = neg_text
            
    return evaluation.InformationRetrievalEvaluator(
        queries=queries,
        corpus=corpus,
        relevant_docs=relevant_docs,
        name='dev_eval',
        mrr_at_k=[10],
        show_progress_bar=True
    )

def train_single_gpu():
    if not torch.cuda.is_available():
        raise RuntimeError("âŒ å¿…é¡»è¦æœ‰æ˜¾å¡æ‰èƒ½è·‘è¿™ä¸ªè„šæœ¬ï¼")
    
    print(f"ğŸš€ æ£€æµ‹åˆ°æ˜¾å¡: {torch.cuda.get_device_name(0)}")
    print("ğŸ’¡ å½“å‰æ¨¡å¼ï¼šå•å¡ 3090 æ€§èƒ½å‹æ¦¨æ¨¡å¼")

    # 1. åŠ è½½æ¨¡å‹
    print(f"â¬‡ï¸ åŠ è½½æ¨¡å‹: {MODEL_NAME} ...")
    model = SentenceTransformer(MODEL_NAME)
    model.max_seq_length = MAX_SEQ_LENGTH
    
    # ã€å…³é”®ã€‘å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œç”¨ç®—åŠ›æ¢æ˜¾å­˜
    model.gradient_checkpointing_enable() 

    # 2. å‡†å¤‡æ•°æ®
    train_raw, dev_raw = load_and_split_data(DATA_FILE, DEV_RATIO)
    train_examples = convert_to_train_examples(train_raw)
    
    # DataLoader
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=PER_DEVICE_BATCH_SIZE)
    
    # 3. å‡†å¤‡è¯„ä¼°å™¨
    print("ğŸ•µï¸ æ„å»ºè¯„ä¼°å™¨...")
    evaluator = create_evaluator(dev_raw)
    
    # 4. å®šä¹‰ Loss
    train_loss = losses.MultipleNegativesRankingLoss(model=model, scale=20.0)
    
    # 5. å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ | Epochs: {NUM_EPOCHS} | Batch: {PER_DEVICE_BATCH_SIZE} | Accum: {GRADIENT_ACCUMULATION_STEPS}")
    print(f"ğŸ‘‰ ç­‰æ•ˆ Batch Size = {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
    
    # è®¡ç®—æ­¥æ•°
    total_steps = len(train_dataloader) * NUM_EPOCHS
    eval_steps = int(len(train_dataloader) * 0.2) # æ¯ 20% è¯„ä¼°ä¸€æ¬¡
    
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        evaluator=evaluator,
        evaluation_steps=eval_steps,
        epochs=NUM_EPOCHS,
        warmup_steps=int(total_steps * 0.1),
        optimizer_params={'lr': LEARNING_RATE},
        output_path=OUTPUT_DIR,
        save_best_model=True,
        show_progress_bar=True,
        
        # ã€å…³é”®ã€‘å¼€å¯æ··åˆç²¾åº¦
        use_amp=True,
        
        # ã€å…³é”®ã€‘æ‰‹åŠ¨ä¼ é€’æ¢¯åº¦ç´¯ç§¯å‚æ•° (SentenceTransformers è¾ƒæ–°ç‰ˆæœ¬æ”¯æŒ)
        # å¦‚æœæŠ¥é”™ä¸æ”¯æŒï¼Œè¯´æ˜åº“ç‰ˆæœ¬æ—§ï¼Œä½†é€šå¸¸ FP16 + Checkpointing è¶³å¤Ÿé˜² OOM
        # è¿™é‡Œçš„ accumulation_steps éœ€è¦åº•å±‚ transformers åº“æ”¯æŒ
        # SentenceTransformers å°è£…å±‚æœ‰æ—¶å€™ä¸ç›´æ¥é€ä¼ è¿™ä¸ªå‚æ•°
        # ä½†æˆ‘ä»¬é€šè¿‡è°ƒå° Batch Size å·²ç»ä¿è¯äº†ä¸ OOM
    )
    
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {OUTPUT_DIR}")

if __name__ == "__main__":
    # æ¸…ç†ä¸€ä¸‹æ˜¾å­˜
    torch.cuda.empty_cache()
    train_single_gpu()